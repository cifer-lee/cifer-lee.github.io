<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on</title><link>/posts/</link><description>Recent content in Posts on</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Sat, 22 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>有氧运动和无氧运动的区别</title><link>/posts/anaerobic-aerobic-exercise/</link><pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate><guid>/posts/anaerobic-aerobic-exercise/</guid><description>人体供能 三大 &amp;ldquo;储能&amp;rdquo; 物质:
糖原(碳水化合物) 脂肪(甘油三酯) 蛋白质 中学生物有学过, 细胞活动所需的能量的直接提供者是 ATP. 糖, 脂肪, 蛋白质供能最终都是重重分解转化为 ATP 来供能.
有氧运动和无氧运动的区别 有氧运动就是低强度的持续时间长的不断有氧气参与供能的运动, 无氧运动则是高强度高爆发性的没有氧气参与供能的运动. 这个说法你不难在网上很多地方查到, 但是很少有人对这个说法又进一步的辨析, 比如说俯卧撑是无氧运动, 但是不是只要我一直不断的做, 就也会变成有氧?
答案很明确, 不是. 虽然一直不断的俯卧撑看起来满足了有氧运动持续时间长的特点, 但是更重要的是运动强度, 俯卧撑的特点在于每次下落和撑起时肌肉的爆发性的收缩舒张, 这个过程发生在一瞬间, 氧化供能是跟不上的. 当然随着俯卧撑的时间加长, 有氧供能也一定会参与进来, 但参与进来也不是因为要给肌肉瞬间的收缩供能, 而是给俯卧撑过程中其他的非爆发型的环节功能, 比如加快的呼吸, 持续高翘的屁股等.
实际上任何运动过程都不是纯粹的有氧或者无氧, 有氧和无氧过程从一开始就都会存在, 但关键看运动过程中谁发挥了关键作用. 俯卧撑无氧运动起到关键作用, 因为这个运动的关键点就是卧撑瞬间的力量爆发, 长跑一定也有无氧过程, 否则就不会跑着跑着肌肉酸痛1, 但是令长跑持续下去的关键还是背后糖原和脂肪源源不断的氧化供能.
锻炼肌肉会有助于减肥吗? 答案是肯定的.
减肥是为了减掉多余的脂肪, 脂肪组织是负责储能的, 而肌肉组织却是高能耗组织2, 维持肌肉块所需的能力哪里来呢? 刚吃的饭, 体内储存的糖原都是短暂的, 关键还是要靠脂肪来作为能量后盾.
然而人体是聪明的, 人身上的肌肉只要不经常用, 这部分肌肉就会首先被拿来祭天3, 肌肉没那么多了, 对脂肪的消耗也就少了. 这也是很多人通过节食来减肥但往往效果适得其反的原因, 人们以为节食后身体不能从食物摄能就会转而从脂肪摄能, 殊不知身体会先从肌肉摄能, 然后才是脂肪, 但是因为肌肉也减少了, 需要维持肌肉形态而消耗的脂肪也减少了, 这两部分效用叠加就可能得到脂肪不减反增的效果.
这里一定要提一句, 我们说的 “肌肉是高能耗组织” 指的是肌肉你一定要不断的用它它才是高能耗组织, 也就是不断的锻炼它, 就算你有一腿的肌肉但是不走路也不会耗多少能量一样.</description></item><item><title>Gold Mining</title><link>/posts/gold/</link><pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate><guid>/posts/gold/</guid><description>Relating concepts Gold grade Gold grade is a term used in gold mining, and should be used as a measure of the quality of gold ore – that is the raw material obtained from mining.
The grade of mined, raw gold ore is measured in grams of gold per tonne of ore. High grade gold ore contains more gold per tonne; this means that more gold is obtained from every tonne mined.</description></item><item><title>以太坊智能合约快速上手</title><link>/posts/ethereum-smart-contract-quickstart/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>/posts/ethereum-smart-contract-quickstart/</guid><description>这两天学习以太坊合约开发, 不得不说以太坊社区是真 &amp;ldquo;繁荣&amp;rdquo;, 网上的教程多得数不胜数, 为什么繁荣加引号呢, 因为从这些教程的质量来看就知道, 以太坊社区的繁荣是各自为营, 社区里各式各样的团队各怀鬼胎的搞以太坊的生态建设, 而实际上根本目的都是为了推销自己的 dapp, 试图扩大自己在生态圈里的虚名，做以太坊社区 V 神之下的老二. V 神显然也不想做领袖角色, 社区就让他自由发展吧!
可是官方文档你好歹整好点, 我在官方想找个智能合约的 quickstart 都找不到, 官方文档说两句就丢你给个 remix 或者 truffle 等工具链接让你用那些工具写智能合约. 这不是我想要的, 我想学习的是不借助任何那些花里胡哨的 IDE 怎么把合约部署到主链上, 官方文档却不教你这一点, 没有这个信息, 即使有可能也是某个开发者的个人博客上有介绍(可能也是自己研究之后做个笔记), 而那些花里胡哨的工具的文档只会介绍自己的工具怎么样, 更不会跟你说底层原理.
不得已, 我只能多花了些时间把智能合约的开发部署捋了一遍. 下面我会摈弃虚华的外表, 让你能以最小的学习成本 + 最快的速度完成一个智能合约并发布.
开发环境搭建 环境方面我们需要:
一个趁手的编辑器, 我选 vim 你随意 智能合约的编译器, 我们选择 solcjs 以太坊节点, 我们选择 geth 需要搭建一个测试或者私有网络, 我们选择 rinkeby 测试网络 关于开发环境方面上述 4 点是我摸索出来成本最低最省事而又最接近在主网部署合约的配置, 诸位如果是第一次接触以太坊智能合约开发大可放心按我的方式来.
反观以太坊的官方文档, 看看它是怎么叫我们搭本地开发环境的:
https://ethereum.org/en/developers/local-environment/ 看到了吗, 直接甩给你一堆框架, 谁知道用哪个好, 官方就不能tm多写点自己写个教程吗. 算了, 闲槽少吐, 我们直接开始我们自己的流程.</description></item><item><title>2020 年终总结</title><link>/posts/year-in-review-2020/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>/posts/year-in-review-2020/</guid><description>2020 年终总结 个人性格 始终还是优柔寡断的性格, 2021 年一定要改变这个性格.
工作 对管理有了一些思考, 一个管理者应该具备真正的资源把控能力, 而不应该单纯的就是派活给团队成员, 更不应该是 &amp;ldquo;拿着鞭子&amp;rdquo; 在团队成员后面催工的剥削者. 优秀成熟的管理者应当同时服务好上级和下属, 按时保质的完成上级的任务是当然的, 但是也不要牺牲下属的舒适度. 否则就可能会造成下属在巨大的压力下无法完成任务, 自己也无法向上级交付的尴尬. 个人技能 对 Tor 网络与代理的认识更深了 匿名主机, 匿名域名. 知道了如何在网络上匿名创办网站 加深了对 DNS 的了解 学习了 Rust 语言. 语言很不错, 就是条条框框有点多, 不适合我这种不需要编译器来保证内存安全的高手. 重新拾起了 python 并用 flask 做了个网站. 在一起体会了 &amp;ldquo;人生苦短, 我用 python&amp;rdquo; 的真谛, 因为同样地网站我先用 Rust 语言现学现做花了 10 倍多的时间 学习了 mongodb 数据库的使用, 以后我尽可能都不会再用 mysql 了 了解了如何通过创建 SSH tunnel 来翻墙 学习了怎么创建一个 HTTPS 服务 投资 今年开始学着买了基金和股票, 基金和股票的持仓部分取得了勉强胜过通胀的收益 开通了汇丰香港卡, 美股和港股也有所涉及.</description></item><item><title>Rust 里的单例模式</title><link>/posts/rust-singleton/</link><pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate><guid>/posts/rust-singleton/</guid><description>Rust 要想搞个单例模式还真是曲折.
单例模式说到底就是全局变量, 在别的语言很容易实现, 但是在 rust 里则诸多不便, 因为 rust 这门语言从设计上就是要避免使用全局变量的. 实际上 Rust 里就没有全局变量的概念, 只有生命周期的概念, 全局变量某种意义上也就是具有整个运行时周期的静态变量, 如果我们真的要定义全局变量, 那就用 static 关键字. 后面我们姑且不论全局变量和静态变量的区别, 统一用静态变量的说法.
Rust 的 static 关键字和 C/C++ 语言一样, 代表变量的生命周期是整个运行时, 但是相比 C/C++ rust 的 static 有一个限制: 必须使用编译时期能够确定的值初始化. 刚看到着点有点想吐槽, 这不就跟 const 常量有点像了么, 不过一想这应该也是 rust 为了安全考虑, 防止运行时有多处地方想要初始化这个静态变量. 但是给我们造成的麻烦可就不小了, 具体麻烦程度取决于我们需不需随后在运行时修改这静态变量的值, 需要分别讨论.
如果我们运气好不需要运行时修改静态变量, 意味着变量声明不用加 mut, 那就很好, 虽然这种情况其实直接用 const 就行了&amp;hellip; 但总之看起来至此需求满足了, 讨论可以结束? 很遗憾并没有.
往往我们用单例模式都是要和系统资源打交道的, 比如网络连接, 文件描述符等, 这些系统资源当然本身是不支持同步访问的, 需要上层有一种机制保证对他们的访问是同步的. 对于 unmutable 的静态变量, rust 会编译时检查这个变量类型是否本身支持同步访问, 支持则已, 不支持则会报错. 不愧是 rust, 多么的严谨哦~ 那 rust 是怎么检查的呢?</description></item><item><title>AWS Implementation of Redis Cluster</title><link>/posts/aws-redis-cluster/</link><pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate><guid>/posts/aws-redis-cluster/</guid><description>AWS redis cluster topology Refer to [2]
Number of connections to master and slave nodes For each node there are two connection related metrics: NewConnections and CurrConnections.
NewConnections, AWS ElastiCache derives this by subtracting two consecutive samples of the total_connections_received stats of a redis node. CurrConnections, this metric is derived directly from the redis stats connected_clients which contains the number of client connections(excluding the connections from replicas) Redirect &amp;gt; Normally slave nodes will redirect clients to the authoritative master for the hash slot involved in a given command, however clients can use slaves in order to scale reads using theREADONLY command.</description></item><item><title>浏览器追踪技术与防范</title><link>/posts/browser-tracking/</link><pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate><guid>/posts/browser-tracking/</guid><description>Cookie 这里说的就是我们通常所熟知的 cookie，很多第三方公司就是借助这种 cookie 实现追踪的。比如网站 A，B 都使用了 DoubleClick 的 js 脚本，DoubleClick 的脚本在用户访问网站 A 时被加载并埋下 cookie，下次用户访问网站 B 时这个信息会上报回给 DoubleClick。这种方式实现起来简单，防范也简单，定期清理 cookie 就能定期的中断跟踪，或者我们也可以直接禁止第三方 cookie 就好了，不过这样一来一些非跟踪的第三方服务比如 google analytics/adsense 也会被误杀。
Supercookie Supercookie 应该是一个统称，不同于普通的 cookie，supercookie 通过各种奇技淫巧达到让用户难以清除甚至无法清除的目地。比如利用 flash 插件存储 cookie 数据，这样一来 cookie 数据就存放于 flash 插件的存储区，而浏览器一般是没有提供插件数据清理功能的，这就阻拦了大部分用户清理 cookie；另外还有一种方式叫做 Image hack，它利用了浏览器默认会缓存图片的行为，给你生成一张 100 像素的每个像素的颜色对你也是固定的小图片，以此来标记你，当浏览器下次发出请求时，一段简单的 js 代码就能够读出缓存中这张图片的像素颜色特征。
这两种方式可算是奇淫，但仍然是把特征信息存在用户电脑上，只要用户想，还是能够清除他们的。然而下面这第三种方式就真的无法清除了。
第三种方式是直接由 ISP 厂商参与，在中途给你插入 cookie 信息。具体来说就是 ISP 厂商分析你的流量，发现是 HTTP 请求，就使用你的网络接入信息给你生成 cookie 信息。这一勾当最早是由美国 Verizon 电信公司实践的，Verizon 为了更好的服务自己的那些广告主们，用这种方式跟踪用户。虽然这种方式使得我们无法清除 cookie，但因为这种方式需要 ISP 分析发现 HTTP 流量，所以只要我们尽可能的只访问 HTTPS 服务（HTTPS Everywhere 插件能够帮助我们尽可能多的走 HTTPS 链路），还是能够有效防止被通过这种方式跟踪的。</description></item><item><title>对解释器与 JIT 的一点思考</title><link>/posts/jit/</link><pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate><guid>/posts/jit/</guid><description>我一直知道解释器与编译器的区别. 编译器是事先将代码编译成机器码, 然后直接送进内存让 cpu 执行, 解释器则是解释执行代码, 可能会将代码先转换成一种中间码, 但我一直有一个误区就是解释器在解释执行的时候会把源代码或者中间码转成机器码, 也直接交由 cpu 执行, 然而我错了. 解释器是不会把源代码或者中间码转换成机器码的, 源代码或者中间码是直接在解释器内部的虚拟机上执行的.
当我认识到这一点之后, 我首先想到的虚拟机怎么实现的基本操作? 比如不借助 cpu 中的 ALU (加法器), 虚拟机怎么实现加法操作? 比如 2 + 3 这个表达式, 在词法分析后得到 &amp;ldquo;2&amp;rdquo; &amp;ldquo;+&amp;rdquo; &amp;ldquo;3&amp;rdquo; 三个 token, 虚拟机可以知道这是加法, 但是它怎么知道 &amp;ldquo;2&amp;rdquo; 和 &amp;ldquo;3&amp;rdquo; 分别代表多少? 2 + 3 它要怎么计算? 这是虚拟机无法模拟的. 略加思考, 我明白了, 试想一下, 虚拟机是什么语言写的? 虚拟机又是运行在哪里?
对的, 虚拟机本质上还是一坨直接运行于 cpu 之上的机器码, 当它拿到 &amp;ldquo;2&amp;rdquo; &amp;ldquo;+&amp;rdquo; &amp;ldquo;3&amp;rdquo; 的时候, 就直接讲这道题交给 cpu 的 ALU 去完成了, 然后取得结果作为它所解释的程序的运行结果.
那么 JIT (Just in Time) 是什么呢? JIT 实际上还是将源代码编译成机器码交由 cpu 执行.</description></item><item><title>对协程的一点认识</title><link>/posts/coroutine/</link><pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate><guid>/posts/coroutine/</guid><description>协程的调度 我们知道线程是 CPU 的基本调度单元，线程调度靠的是时钟中断.
协程是执行于线程之内的更细粒度的执行单元，他的调度无法依赖时钟中断，而是要靠一个用户态的调度器，这个调度器可以是抢占式或非抢占式，抢占式调度器需要语言的运行时支持，据我所知只有 erlang 实现了协程的抢占式调度。大部分的协程实现都是非抢占式调度，非抢占式调度实际上是依靠协程之间相互让权 (yield) 来得到执行。
在非抢占式协程下，不存在协程同步问题。而在抢占式协程下则语言我们也考虑数据竞争，协程同步问题。
协程的好处 协程的一个典型应用是用在生产者 - 消费者问题中. 我们知道生产者 - 消费者问题也可以用多线程解决, 生产者线程和消费者线程共享一个上了锁的消息队列, 靠内核调度这两个线程执行来完成生产和消费过程, 然而这里有两个不足之处:
靠内核调度线程, 存在线程切换开销 消息队列加锁, 存在锁竞争和线程同步问题 内核调度线程的时机不确定, 如果在调度消费者时队列中没有消息, 消费者只能什么也不干就退出, 白白浪费了一次调度而如果用协程解决的话, 就不存在上述问题. 首先生产者和消费者协程位于统一线程里, 不存在线程切换的开销; 其次由于是单线程, 无需加锁, 也就不存在锁竞争问题; 最后由于协程之间的执行是靠主动让权 (yield), 我们可以在实现的时候仅当队列不空时才让权给消费者, 同理消费者仅当队列不满时才让权给生产者.
另外使用协程还有一个好处就是能够以看似同步的方式写异步的代码.
协程实现 要实现协程就需要自己在线程中维护第二层栈空间 (第一层是线程自己的栈空间), 因为线程的切换内核会为我们将当前上下文 (主要是各个寄存器的值) 保存在线程栈空间中, 现在由于线程需要自己调度协程, 所以线程需要为每个协程维护栈空间, 好在协程切换时保存协程的上下文.
这里需要线程能够获去到当前执行上下文, 很多操作系统内核会提供相应的系统调用, 实现方式其实也很简单就是写一段内嵌的汇编获取各个寄存器的值.
在 C/C++ 中, setjmp/longjmp 帮我们完成了这个任务. 关于其 setjmp/longjmp 的实现原理, 这里有篇 Google 排名第一的文章 讲的很清楚. C/C++ 中实现协程当然也可以不借助 setjmp/longjmp 而自己去实现上下文的获取和维护, Google 可以搜到不少.</description></item><item><title>DPoS 核心概念</title><link>/posts/dpos/</link><pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate><guid>/posts/dpos/</guid><description>本文基于 BM 的唯二的两篇阐述 DPoS 机制的文章, 第一篇文章是 BM 首次提出 DPoS 共识机制, 第二篇是 BM 后来对 DPoS 机制补充的白皮书. 两篇文章的链接见文末.
最近精读了这两篇文章, 从中提炼出了以下我认为是最核心的内容.
见证人选举与调度 DPoS 里最重要的两个部分就是见证人的选举以及调度出块.
见证人的选举靠持币人的投票, 并且是一币一票的规则, 持币量有多少, 投票权重就多大. 但我觉得这不好, 权重不应当随持币量而无节制的增大.
投票选举是 DPoS 与其它共识机制最大的不同点, DPoS 高度依赖投票机制来剔除不好的见证人, 从来维持链条运行在一个健康稳定的状态. 反观比特币等其它共识, 在发生节点作恶的情况时, 所能采取的措施就捉襟见肘了.
调度这块也是与比特币有着很大的不同, 比特币中是竞争出块, 而在 DPoS 中块由谁出则是被安排的明明白白. 在一个任期内经过选举选出一批见证人, 然后这批见证人就公平的轮流出块了, 任期的长短是可以被协商的. 在任何时候, 持币人都可以重新投票, 下一个任期会根据最新票数重新选举一批见证人. 在最初的 DPoS 机制中, 当这批见证人轮流出完一轮块之后, 他们的顺序还会被打乱, 不过在 EOS 中, 这个规则已经取消了, 后面会说到.
最长链准则 (一致性保证) 和比特币一样, DPoS 里见证人出块也遵守最长链准则, 这是所有节点的基本共识, 是一种弱一致性策略.
第二篇文章的前半部分讨论了几种可能的分叉情况, 比如消息延迟, 网络分区, 甚至是存在少量作恶节点等, 并说明了在最长链规则下这些问题会迎刃而解. 但这些基本上都是最长链准则所解决的, 这在比特币时代就是 work 的, 并不是 DPoS 的创新点.</description></item><item><title>C++ 的左右值与左右值引用</title><link>/posts/c-plus-plus-reference/</link><pubDate>Sun, 29 Apr 2018 00:00:00 +0000</pubDate><guid>/posts/c-plus-plus-reference/</guid><description>左值与右值 C++ 中左值和右值的概念来源于 C, 在 C 中左值和右值的区别很简单, 能出现在赋值号左侧的就是左值, 否则就是右值. 比如变量是左值, 字面常量或者 const 定义的常量是右值.
然而在 C++ 中, 左值和右值的区别就不再是那么简单了, 甚至和 C 还会有冲突, 比如在 C 中 const 定义的常量对象是右值, 而在 C++ 中却是左值.
实际上在 C++ 中左值和右值的情况非常复杂, 有时区分他们也是非常困难的. Scott Meyers 大师在其 Effective Modern C++ 一书所说的不失为一个好方法, 在理解这句话之前, 我们一定要有一个意识, 就是左值和右值是表达式的属性, 代表着表达式的运算结果是左值还是右值.
A useful heuristic to determine whether an expression is an lvalue is to ask if you can take its address. If you can, it typically is. If you can’t, it’s usually an rvalue.</description></item><item><title>如何在应用层控制最大客户端连接</title><link>/posts/controlling-client-connections/</link><pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate><guid>/posts/controlling-client-connections/</guid><description>当有客户端连接, 而程序中没有去处理时, select 就回持续不断的返回这个文件描述符可写, 例如, 下面是我以前写的一段有 bug 的程序:
int csocks[MAX_CONNECTION]; memset(csocks, -1, MAX_CONNECTION * sizeof(int)); FD_SET(sock, &amp;amp;rset); while(1) { if (select(FD_SETSIZE, &amp;amp;rset, NULL, NULL, NULL) &amp;lt;= 0) { return ; } if (FD_ISSET(sock, &amp;amp;rset)) { // looking for an unused socket for (int i = 0 ; i &amp;lt; MAX_CONNECTION; ++i) { if((-1 == csocks[i]) &amp;amp;&amp;amp; (-1 != (csocks[i] = accept(sock, NULL, NULL)))) break; } } } 这段程序里, sock 是一个侦听套接字, 负责侦听客户端的连接, 一有连接就会去调用 accept 来接受客户端的连接 &amp;mdash; 当然, 这是有条件的, 那就是能够接收的最大的客户端数量是 MAX_CONNECTION, 由上面的程序里可以看到, 当连接的客户端的数量已经超过了 MAX_CONNECTION 时, 将不会再接受任何连接.</description></item><item><title>C 语言宏的展开与字符串化宏和符号连接宏</title><link>/posts/c-macro/</link><pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate><guid>/posts/c-macro/</guid><description>C 语言由于没什么高级的特性, 所以现有的特性被玩的各种精. 宏展开就是很值得品味的部分.
递归展开问题 宏定义语句是这样的:
#define identifier token-sequence
在具体的宏展开过程中, 遇到标识符时, 此标识符会整个的被使用 token-sequence 展开, 如果 token-sequence 中还包含有其他的被定义的宏标识符, 也都会相应的被展开. 但是显然, 已经展开过的标识符如果再次出现, 则维持原样不变, 不会再次展开, 否则就递归个没完了.
比如说有如下代码段:
#define x y #define y x x 对其执行 gcc -E 预处理时得到的结果是 x, 这里发生了两步替换, 首先 x 被展开成 y, 然后因为 y 也被定义为宏, y 又展开成 x. 注意此时, 由于 x 这个标识符已经展开过, 这里是第二次出现, 所以不会被再次展开成 y. 不然就没完没了了.
这部分可以参见 K&amp;amp;R C, A.12 中有这么一段话:
In both kinds of macro, the replacement token sequence is repeatedly rescanned for more defined identifiers.</description></item><item><title>比特币的多重签名技术与实践</title><link>/posts/bitcoin-multisig/</link><pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate><guid>/posts/bitcoin-multisig/</guid><description>Multisignature scripts set a condition where N public keys are recorded in the script and at least M of those must provide signatures to unlock the funds. This is also known as an M-of-N scheme, where N is the total number of keys and M is the threshold of signatures required for validation
&amp;lt;精通比特币&amp;gt; 多重签名首次规范化提出是在 BIP11, 它添加了一种新的交易类型 OP_CHECKMULTISIG, 这种交易的锁定脚本是如下这种格式:
M &amp;lt;Public Key 1&amp;gt; &amp;lt;Public Key 2&amp;gt; .</description></item><item><title>论铸币制度与比特币多中心化</title><link>/posts/bitcoin-currency/</link><pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate><guid>/posts/bitcoin-currency/</guid><description>货币起源 在人类文明发展史上, 由于物物交换的不便而产生了一般等价物, 金银等金属由于其不易损耗而又能够分割并重熔的特性成为备受认可的一般等价物 &amp;mdash; 货币.
铸币制产生 金属作为货币并没有改变贝壳, 石头作为货币时的问题. 贝壳作为货币时, 我们可以每天不种谷物只管去海边捡贝壳然后拿回来跟别人换谷物. 金属成为货币后, 虽然不是随便捡的, 但是仍然有很多的人有锻造金属的能力, 一时间市场上形形色色的金属货币横飞, 而铸币的重量和各金属成分的含量又非常人所能辨别, 导致市面流通的金属货币鱼目混杂, 假币横飞.
权利收敛 正式由于铸币的重量和金属成分常人无法检测, 便产生了铸币监制局, 其它各铸币机构的币一定要经过监制局打上监制局的标记才能被认可流通. 后来由于铸币机构仍然存在不干活给自己造币就能致富的可能, 索性铸币的权利也被中央化, 民间机构不再具备铸币的权利.
比特币的现状 铸币权的收敛, 发展至今, 最大问题就是中央信誉的问题, 古今中外, 所有中央机构所铸的币都是在缩水的. 然而如果没有中央机构, 又如何解决民间铸币带来的假币横飞问题呢? 如果人人都能铸币, 谁还会去劳动? 文明如何进步?
比特币的想要将铸币权重新赋予每个人, 但发展至今, 显然这一初衷已经失败了. 比特币的铸币权已经被几个中心所控制, 而且现在市面上已经出现了各种比特币的分叉币. 联系金属铸币的发展历程来看, 这一幕很熟悉不是吗?
后记 哈耶克在 &amp;lt;货币的非国家化&amp;gt; 中提出, 货币也应该像商品一样, 同样的商品在市场上有多家生产商, 市场会告诉哪家的商品卖的更好. 比特币以及其众多的分叉币目前正处在这样一个处境. 当初金属铸币百家齐鸣时, 没能由市场决定存留, 在比特币时代, 能够达成这一愿景实现 货币的非国家化 呢?
做到了, 比特币才是比特币, 做不到, 比特币只是另一个金银罢了.</description></item><item><title>2017 年终总结</title><link>/posts/year-in-review-2017/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/posts/year-in-review-2017/</guid><description>学习 学会了使用 python 写简单的工具 从盛科辞职, 加入了滴滴 研究数字货币, 研究的很深. 投资了 HPB, BTS 和 STEEM 人生 结婚 在一沿海二线城市买了方, 买的时候已经不便宜了 浪费时间的瞎折腾 emacs, orgmode 为了使用 orgmode, 不得不学习折腾 emacs, 要是早点发现 onenote 就好了. 今后严格杜绝在这样的事情上浪费生命!</description></item><item><title>比特币隐私加固 - CoinJoin 技术简析</title><link>/posts/bitcoin-coinjoin/</link><pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/bitcoin-coinjoin/</guid><description>思想 由 @gmaxwell 在 CoinJoin: Bitcoin privacy for the real world 一文提出. 核心思想就是利用比特币的一笔交易中可以有多个输入以及多个输出这一点, 将多笔交易合并, 使得让人难以分辨哪笔输入对应哪笔输出, 进而达到难以追踪某个地址的资金的来源或去向的目的.
CoinJoin 思想的通俗实践是, 当你想要转账时, 可以找到另外一些也想转账的人, 你们分别签名自己的输入共同创建一笔交易. 这就需要掺入一点点中心化来支持, 对于个人钱包来说并不太好实践, 但对于具有中心服务的 web 钱包提供商来说比较容易, web 钱包可以很容易的将多个用户的转账请求合并, 作为一笔交易广播出去, 比如 blockchain.info.
CoinJoin 不需要更改比特币固有协议, 后续的许多旨在提高交易私密性的技术都是基于 CoinJoin 的思想.
实现 前面说 blockchain.info 有采用了 CoinJoin, 实际上 blockchain.info 实现的是 Shared Coin &amp;mdash; 一个对 CoinJoin 思想的最简单的实现, 当我们在 blockchain.info 上创建交易时, blockchain.info 会自动将其与其他人发起的交易合并成为一笔.
不过 Shared Coin 后来被研究出交易中的记录仍然是可追溯的, 因此 blockchain.info 下线了 Shared Coin 技术: https://www.coindesk.com/blockchains-sharedcoin-users-can-identified-says-security-expert/
其他对 CoinJoin 思想的实现技术还有 Dark Wallet, CoinShuffle, 达世币中的 PrivateSend 等等.</description></item><item><title>关于 Python 中的 time 与 datetime 模块</title><link>/posts/python-time-datetime/</link><pubDate>Sun, 26 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/python-time-datetime/</guid><description>在 Python 中处理时间有两个库可用: time 和 datetime, 这两个模块曾经在很长一段时间里困扰着我, 我觉得这是 Python 又一处矛盾的地方 (最大的矛盾是 3 和 2 不兼容), 因为一门对开发者友好的语言应该直接提供最好用的库, 而不是让开发者去做选择.
看看 time 与 datetime 的区别, 以及它们的设计用意, 其实从 Python 标准库手册上就能看出来, time 模块位于 &amp;ldquo;16. Generic Operating System Services&amp;rdquo; 这一节, 处于这一节的还有 os, getopt 等模块, 而 datetime则位于 &amp;ldquo;8. Data Types&amp;rdquo; 这一节. 因为 time 是被开发来包裹系统调用的 &amp;mdash; 也就是 &amp;lt;time.h&amp;gt; 头文件那些方法, datetime 才是开发来专门作为时间工具库的. 实际上 time 模块是 C 写的, datetime 模块则是 python 写的.
既然是专注于时间处理, 又不需要特意去对应 &amp;lt;time.h&amp;gt;, datetime 模块自然是功能更全一些, 方法更亲民一些, 所以现在的趋势也是使用 datetime 模块.</description></item><item><title>再论个人知识管理</title><link>/posts/knowledge-management/</link><pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/knowledge-management/</guid><description/></item><item><title>FreeRTOS 的链表 vListInsertEnd() 方法笔记</title><link>/posts/freertos-vlistinsertend/</link><pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/freertos-vlistinsertend/</guid><description>在研究 FreeRTOS 内核时, 发现自已一直理解错了 vListInsertEnd() 的意思, 特此记录下.
vListInsertEnd() 这个方法比较迷惑人, 这个方法真正的意思是尾插, 而不是插到链表的尾部. 这两者的意思是不一样的.
那究竟什么是尾插呢? 我们应该知道头插, 给定一个链表节点 A, 头插就是说新的节点要插在 A 的后面, 而尾插则是说新的节点要插在 A 的前面. 这就是尾插的意思: 把给定的节点当作尾部.
FreeRTOS 的链表结构 List_t 中有一个成员叫做 pxIndex, 这个成员是用来遍历这个链表用的, 具体来说就是在同优先级任务链上, 依次取得下一次任务以达到同优先级任务共享 CPU 时间的. List_t 结构中还有一个成员叫做 xListEnd, xListEnd 是不包含有效数据的节点, 它标识着这个链表的真正的尾部.
之前令我误解的一点就是, 我总以为 vListInsertEnd() 就是将新的节点插到整个链表的尾部, 也就是 xListEnd 的前面. 但是 vListInsertEnd() 却是将新的节点插入到 pxIndex 前面. 我想了好久没明白这什么意思, 直到我看到 listGET_OWNER_OF_NEXT_ENTRY(pxTCB, pxList) 这个宏我才明白, 这个宏就是每次调度器选择下一个任务时所调用的, 这个宏会每次将 pxIndex 更新为它后面的节点. 所以, 为什么 vListInsertEnd() 是将新的节点插入到 pxIndex 前面也就解释的通了, 这是为了更公平的让各个任务共享时间片. 比如说当前有如下的任务链表, 每个任务的优先级相同:</description></item><item><title>通俗地解释 CGI, FastCGI, php-fpm 之间的关系</title><link>/posts/cgi-fastcgi-php-fpm/</link><pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/cgi-fastcgi-php-fpm/</guid><description>这要了解一点万维网 (WWW) 的历史, 才能更好地了解个中关系.
早期的网站基本都是静态的, 那时候的 web server 几乎所有工作就是给访问者提供静态资源, 网站与访问者之间缺乏交互. 后来随着 WWW 的发展网站变得交互性强了起来, 交互性强了也意味着 web server 端的业务逻辑复杂了起来, 不再是简单地解析 url, 定位并返回用户请求的资源, 而是要处理很多用户请求的动态资源以及许多复杂的业务, 这些工作都交给 web server 来做是不现实的, 因为单纯作为 web server 是不知道也不应该关注业务的.
于是 CGI 出现了, 它使得 web server 可以把复杂的业务逻辑交给 cgi 脚本程序来做, CGI 协议定义了 web server 与 cgi 程序之间通信的 context, web server 一收到动态资源的请求就 fork 一个子进程调用 cgi 程序处理这个请求, 同时将和此请求相关的 context 传给 cgi 程序, 像是 path_info, script path, request method, remote ip 等等&amp;hellip;
但是显然每次来个请求 web server 就去 fork 子进程是很低效的, 在网站访问量逐渐增大时网站性能问题日益凸显.</description></item><item><title>什么是比特币的链上 (on-chain) 与链下 (off-chain) 交易, 以及往交易所充币后发生了什么</title><link>/posts/bitcoin-on-off-chain/</link><pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/bitcoin-on-off-chain/</guid><description>on-chain on-chain 就是比特币的常规交易方式: 给我一个比特币地址 (公钥), 我用客户端创建交易发送比特币给你, 这笔交易在全网广播, 被确认, 被打包进区块. 显然, 交易是直接发生在链上的.
off-chain 至于 off-chain 其实玩过比特币的人也都用过, 我们在交易所里的交易就是 off-chain 交易. 这是如何运作的呢?
A, B 分别在某交易所开户, 交易所会分别为 A, B 生成一对公钥私钥, 但是 A 和 B 都不知道平台给他们生成的私钥, 只知道自己的公钥. 然后, A 和 B 用自己的钱包往平台给他们开的公钥地址里冲值比特币, 注意这个操作依然是 on chain 的.
再然后, A 通过交易所转了 0.5BTC 给 B, 但由于 A 没有私钥, 所以需要交易所拿 A 的私钥去签名并广播这个交易, 然而交易所真的需要去广播这个交易吗? 不需要的, 交易所只需要在自己的数据库里, 将 A 的账户余额 -0.5BTC, 将 B 的账户余额 +0.5BTC. 这一步, 只是交易所自己维护的信息在更新, 没有上链, 所以这个操作是 off chain 的.</description></item><item><title>深入解读同步/异步 IO 编程模型</title><link>/posts/io-programming/</link><pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate><guid>/posts/io-programming/</guid><description>所谓 &amp;ldquo;同步&amp;rdquo; 和 &amp;ldquo;异步&amp;rdquo; 是从调用者的角度来说的. 如果调用者不得不等待 IO 完成才能执行后续的工作, 那就是同步; 否则, 就是异步. 这是我对 &amp;ldquo;同步&amp;rdquo; 和 &amp;ldquo;异步&amp;rdquo; 的定义, 这个定义清晰精炼, 巧妙的帮我们把 &amp;ldquo;理解什么叫做异步&amp;rdquo; 这项工作简化成了 &amp;ldquo;理解什么叫做 IO 完成&amp;rdquo;.
在 *nix 系统中, IO 操作分为两个阶段. 第一阶段是从用户空间发起请求到数据真正就绪的等待阶段, 第二阶段是数据就绪后从用户空间或者内核空间拷贝给对方的数据拷贝阶段. 只有这两个阶段都完成了, 才叫做 &amp;ldquo;IO 完成&amp;rdquo;.
如果看过圣书 Unix Network Programming Volume 1 , 就知道 Richard 介绍了 5 种 IO 模型, 下面我们按照上面的定义给这 5 中模型分个类.
Blocking IO 这个模型是最简单的, 程序流调用 read/writei, 如果运气好正好有数据, 就进行 IO 第二阶段, 否则就卡在第一阶段等数据就绪. 当第二阶段结束 read/write 返回后, 继续执行后面的程序.
/* processing work A */ read(fd, buf, size); /* blocked here */ /* continue processing work B */ 显然这个模型是同步的, 程序流必须等 IO 两个阶段都完成了, 才能得以执行后续的工作.</description></item><item><title>说一说重放, 重放保护, 以及分叉期间我们该怎么做</title><link>/posts/bitcoin-replay/</link><pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate><guid>/posts/bitcoin-replay/</guid><description>何为重放 先说重放, 重放指的是分叉过后, 由于新链和旧链所运行的协议可能完全兼容, 导致在旧链上发生的交易拿到新链上也是合法的. 以本次的即将上演的 BTC/BTG 分叉为例:
在高度 A 左右我们生成了两个新地址 a, b. 然后在高度 B 的时候, BTC 链分出了 BTG 链 (也就是三角形那条, 图里忘了标了). 重放是怎么发生的呢?
假如我们现在在 BTC 链上发起交易 a -&amp;gt; b, 由于 BTC 网络的交易是明文全网广播的, 所以 b 很容易在 BTC 网络截获到这个 a -&amp;gt; b 交易广播, 然后 b 只要把这个广播全文拿到 BTG 网络上再进行一次广播, 这就达成了重放. 可以说是 b 对 a 做了一次重放攻击.
同理, 如果 a -&amp;gt; b 的交易最初是在 BTG 链上发起的, 那么这次交易广播也很容易被拿来在 BTC 链上重放. 这就是双向重放.
总之重放的后果是显而易见的, 如果上例中 a 是你, b 是我, 那么原本在另一条链上属于你的钱, 就会被一并转给我.</description></item><item><title>闲聊即将到来的 segwit2x 分叉</title><link>/posts/segwit2x/</link><pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate><guid>/posts/segwit2x/</guid><description>早些时候由矿工主导的 &amp;ldquo;纽约共识&amp;rdquo; 达成了 segwit2x 方案, 约定在 2017 年 9 月以前启动隔离见证, 并在随后的 3 - 6 个月内启动 2mb 扩容. 所以 segwit2x 这个名字起的有意思, 看着好像跟 segwit 2.0 版一样, 实际上 segwit2x 指包含两部分, segwit 是隔离见证, 2x 便是指区块容量扩至 2mb.
既然达成了 &amp;ldquo;共识&amp;rdquo;, 为什么还说会有分叉呢, 原因是这一 &amp;ldquo;共识&amp;rdquo; 并没有得到 core 开发组的支持, core 开发组一直以来维护着 bitcoin core 核心代码, 也就是 Satoshi 最初的代码, 在社区里声望一直极高. 虽然 core 向来是支持 segwit 的, 但是并不支持 2x, 所以 11 月份的分叉也就是在 core 开发组和 segwit2x 支持者 (主要是矿工) 之间产生的分叉, 也被称作 1x 与 2x 之争.</description></item><item><title>Electrum 钱包的 sweep 功能小记</title><link>/posts/electrum-sweep/</link><pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate><guid>/posts/electrum-sweep/</guid><description>electrum 是款优秀的 bitcoin 轻钱包, bitcoin.org 的文章里经常有提到它, 可见 bitcoin.org 除了自己的 core 钱包之外还是比较推荐 electrum 的.
使用 electrum 钱包时, 建议的生成私钥的方式是从 seed 生成, 这样所有生成的密钥都是可以从 seed 推算出来的, 对钱包的备份也就简化为对 seed 的备份.
sweep 功能 electrum 有一个 sweep 功能, 刚开始乍一看我惊诧的以为这个功能是销毁钱包, 因为这个功能就在 &amp;ldquo;Private keys -&amp;gt; sweep&amp;rdquo; 菜单下, 看着意思就像 &amp;ldquo;清除所有私钥&amp;rdquo; 一样, 实际上当然不是的, sweep 存在的目的是让我们把其它客户端生成的私钥导入到我们在 electrum 中使用 seed 生成的钱包中.
导入私钥本应该是个很简单直接的事情, 但是由于 seed 方式生成的钱包的特点就是里面的私钥都是可以由 seed 推导出来的确定性私钥, 随便导入一个我们在其它客户端生成的私钥自然是不行的. 因此就引入了这个 sweep 机制, sweep 的本质就是将你在其他客户端上所生成的私钥的所有余额发送到这个钱包中由 seed 生成的某个私钥上.
所以 &amp;ldquo;sweep&amp;rdquo; 指的是 sweep 别的客户端生成的私钥, 而不是 seed 钱包里的私钥.</description></item><item><title>Bitshares 中的账户与权限个人理解</title><link>/posts/bitshares-account-mechanism/</link><pubDate>Sat, 16 Sep 2017 00:00:00 +0000</pubDate><guid>/posts/bitshares-account-mechanism/</guid><description>单一账户模型 在这个模型下, 我们提供一个登录名以及密码, 系统会根据登录名+密码生成确定的私钥, 所以用这种模式创建的账户, 可以在任何设备上用登录名+密码登录.
钱包模型 钱包模型的好处是, 钱包可以包含多个账户, 这些个账户的私钥全都保存在此钱包中, 用钱包在任何地方登陆都能获得你所有的账户. 但是有个不方便的地方是登录需要使用钱包文件, 不如直接输登录名密码来得方便 (较新的客户端可以用 brainkey 恢复钱包, 但 brainkey 还是没有输入登录名密码方便).
钱包模型下支持给钱包中的账户新创建三个私钥, 创建时需要提供一个密码, 新建的私钥是根据账户名+密码生成的, 三个私钥分别控制着账户的三个权限 (具体见下文). 如此一来, 钱包中的这个账户就变得好像和单一账户模型中申请的账户一样了. 这个功能能够方便我们使用账户名和密码在其他设备上登录我们的账号.
权限 bitshares 天然对多重签名有良好的支持, 其设计为将每个账户的花钱的权限分离出来, 我们可以为某个账户配置更多的私钥或者其它账户, 并为这些私钥或者其他账户设定不同的权重, 同时设定一个阈值, 要花这个账户的钱, 需要权重大于设定的阈值才可以.
bitshares 目前划分了三级与账户相关的权限, 上面说的花钱的权限是一级. 另外还有 memo 权限, 创建交易时附加的 memo 可以用单独的私钥签名, 这项权限目前还不支持多重签名, 貌似也没必要. 最后一项是最重要的权限, 即对整个账户的控制权, 这个权限能够修改账户的各种信息, 配置, 包括权限配置, 这项权限当然也是支持多重签名的.
参考 官方这两个链接值得一读
http://docs.bitshares.org/bitshares/user/account.html http://docs.bitshares.org/bitshares/user/account-permissions.html</description></item><item><title>比特币地址, 公钥与私钥的格式以及如何保证比特币不丢失</title><link>/posts/what-is-bitcoin-address-how-to-protect-it/</link><pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate><guid>/posts/what-is-bitcoin-address-how-to-protect-it/</guid><description>我们知道在公钥加密体系中, 由私钥推出公钥很容易反过来却是不可能的, 借助一些伟大的数学算法可以实现这一特性, 比特币使用的算法叫做椭圆曲线算法.
在比特币中私钥就是一个 256bit 的数字, 得到这个数字的方法有很多, 最简单的就是随机法, 比如掷硬币 256 次, 将结果作为私钥. 得到了私钥之后对其执行椭圆曲线算法, 我们就得到了对应的公钥, 公钥是 一对 坐标: (x, y) 以及 (x, -y), 其中 x, y 都是 256bit 的数字, 所以公钥的长度有两种情况:
512bit - 同时记录了 x, y 256bit - 记录 x 以及 y 的符号, 这需要一点点额外的运算来算出 y 公私钥编码 如果用 2 进制, 即便是 16 进制来写出公钥或私钥的话, 那结果是很长的, 比较不方便. 因此我们所见到的公私钥一般都是格式化过或压缩过的.
从 bitcoin core 开始, 比特币客户端使用的格式有 WIF (Wallet Import Format) 以及 WIF-compressed. WIF 格式使用 Base58 编码公私钥.</description></item><item><title>Solidity 的编译器们</title><link>/posts/compilers-of-solidity/</link><pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate><guid>/posts/compilers-of-solidity/</guid><description>solidity 是当下 ethereum 最流行的智能合约开发语言, 语法类似于 javascript. 要使用它写智能合约的话, 我们还需要一个编译器, 用来将 solidity 的代码编译为 EVM 字节码.
solidity 的编译器有多种实现, 下面可以一起看一看.
solc 一般说到 solc 指的都是 ethereum 官方实现的 cpp 版本的 solidity 编译器, 其项目源代码位于 https://github.com/ethereum/solidity. 按照我以往对编译器的印象, 本以为 solc 一定也是像 gcc 那样的庞然大物, 结果发现完全不是, solidity 编译器实际上是个体积很小很简单精巧的工具, 项目源代码算上注释总共也就才 6w 多行. 我们可以克隆这个仓库, 然后自行构建它, 构建出来的主要目标文件就是 solc.
solcjs solidity 的官方文档中还介绍了使用 npm/nodejs 安装的方法. npm install -g solc 之后会得到一个 solcjs 命令, 本来我以为这个 solcjs 实际上是 solc 的前端, 查了一下发现完全不是的: https://github.com/ethereum/solidity/issues/725#issuecomment-233227534, solcjs 本身就是整个 solidity 编译器, solcjsi 项目代码在 https://github.com/ethereum/solc-js, 这个项目是借助 Emscripten 工具直接把 cpp 的代码重写成了 javascript 代码, 于是得到了一个完整的 javascript 版的 solidity 编译器, 我们可以在 nodejs 中直接 require ('solc') 使用它.</description></item><item><title>2016 年终总结</title><link>/posts/year-in-review-2016/</link><pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate><guid>/posts/year-in-review-2016/</guid><description>这是一篇拖了相当久的总结, 2016 年是我动荡不安的一年.
辞职 首先是 6 月份的辞职, 这是我在 Y 工作整整三年来的辞职. 那个时候我的脑中存在着不知从哪听到的一个观念: 三年是一个阶段, 在一家公司工作满三年了就可以跳槽了. 现在回想这个观念实在是没什么道理, 当然如果仅仅是我脑子里存在这个观念, 那还不足以让我辞职的. 不说笑, 驱使我辞职的根本原因还真的就是 &amp;ldquo;世界那么大, 我想去看看&amp;rdquo;. 于我而言, 这句话包含了两层含义, 一是我对工作现状的疲乏, 二是我对外面世界的憧憬.
当时在很长一段时间里, 我的工作是千篇一律的: 不断的为一些变来变去的, 无关痛痒的需求修改代码. 这些工作既繁琐, 又不存在技术壁垒, 久而久之我对这些工作产生了强烈的厌烦感, 我甚至觉得, 嵌入式领域我已经做到头了, 再继续做下去对我也不会有本质的提升, 关于这一点我后来更是坚信不疑.
另外, 从大学算起, 在青岛这个二线城市待了 7 年, 在嵌入式领域做了将近 3 年的我, 对外面更发达的城市, 对于外界的各个未知的领域都充满了渴望和幻想. 我想这才是导致我坚定的辞职的根本原因, 年轻的时候不出去, 什么时候出去呢?
总之辞职是一定的了, 但是在这次辞职的过程中, 暴露出了几个让我现在回想起来都遗憾万分的问题.
首先就是与上级的沟通太少, 这是我这三年里犯得非常严重的一个错误. 公司的同事, 尤其是上级, 在我们的职业生涯中是非常优质的资源, 如果我能利用好这些资源的话, 那么我的职业生涯必定会少走很多弯路, 然而整个在 Y 的三年我都没有重视这些资源. 在我离开 Y 的时候, 我与公司绝大部分同事甚至老板的关系都是很普通的, 这不应该是一个在公司待了三年的老员工的样子.
而且正是由于沟通太少, 我没有想到的是, 在我提辞职的那几天, 上级同时也在计划给我 promotion, 遗憾的是我先于上级的 promotion 提出了辞职, 后面的事情就比较尴尬了.</description></item><item><title>写在入职滴滴的前夕</title><link>/posts/acquired-by-didi/</link><pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate><guid>/posts/acquired-by-didi/</guid><description>明天 (3.21) 就正式入职滴滴了, 这对我来说是非常值得开心的, 然而我这些天却越发的感觉到几丝迷茫.
现在想想, 从上了大学起, 我精神就一直处于一个懒散的状态, 整个大学里没有一点危机意识, 没有去思考我该做什么, 想要什么, 也没有去思考将来要成为什么样的人. 懒散到专业被调剂了也毫不在意的接受, 不去反抗不去挣取.
虽然大学的时候没有少看编程方面的书, 以后做程序员的决心也一直没有改变过, 但是我也仅仅只是心里一直告诉自己 &amp;ldquo;以后要成为出色的程序员&amp;rdquo; 罢了, 至于怎样才能成为, 成为了之后又怎样, 我没有过多思考过, 只是就一味的看书, 像无头苍蝇一样, 没有方向, 没有侧重. 现在想起来, 当是真的是浪费了太多的时间, 导致自己事倍功半.
整个大学时期, 我存在非常严重的几个问题:
认为专业被调到化学无所谓, 认为我靠自学计算机也能学得很好
事实上, 失去了计算机专业的学习环境, 教授环境, 没有前辈的引导, 我对于整个计算机生态体系缺乏系统的认识, 导致了我方向不清, 不能明确自己想要的是什么.
这导致的后果就是我在大学中, 几乎所有的计算机技术书籍都看过, 但是没有一项为我所熟练掌握, 我看过学过的内容过有计算机组成原理, 汇编, C/C++, MFC, Visual C++, Java, j2ee, php, mysql, oracle, flash 动画, html/javascript/css web 前端, android 开发, C#/.Net 开发等等. 但是看这些书时我是迷茫的没有方向的, 我甚至不知道为什么要看这些书, 看完之后能做什么.
闭门造车, 没有和其他人多多沟通</description></item><item><title>记一次 Indigo 与 Ryu 的连接建立问题</title><link>/posts/a-problem-ryu-indigo-conn-setup/</link><pubDate>Sun, 25 Dec 2016 16:03:13 +0000</pubDate><guid>/posts/a-problem-ryu-indigo-conn-setup/</guid><description>之前在研究 Indigo 和 Ryu 的衔接时碰到一个 OpenFlow 连接建立失败的问题, 特此记录下, 希望能够供别人参考.
在了解这个问题之前, 我们先回顾一下 OpenFlow 连接建立过程.
OpenFlow 协议的连接建立过程 根据 OpenFlow 协议标准的陈述我们能够知道, 交换机和控制器之间使用 TCP (或者 SSL) 传输协议, 交换机必须能够主动发起连接 (实际应用中, 连接一般都是都由交换机主动发起), 另外就是所有的 OpenFlow 消息, 都要用网络序 (大端序) 发送 (参见 OpenFlow Spec v1.3 以及 v1.4 的第 7 章)
TCP 连接的建立我们很熟悉了, 就是典型的三此握手过程. 在 TCP 连接建立以后, 交换机和控制器双方在 TCP 连接建立后需要立即发送给 OF_HELLO 消息给对方, 并且 OF_HELLO 必须是双发发送给对方的第一个消息, OF_HELLO 消息同时起到协商 OpenFlow 版本的功能.
当双方都收到了对方的 OF_HELLO 消息并且两边都共同支持一个最小版本, OpenFlow 连接就成功建立了, 接下来控制器就可以向交换机发送其它的消息, 比如一般第一次要发送的就是 OFPT_FEATURES_REQUEST 消息.
遇到的问题 在这部分工作中基于 Ryu 框架我写了个简单的 Ryu 小程序令它与 Indigo 通信, 但是发现似乎连接建立都不成功, 好在 indigo 项目的错误日志部分做的很好, 我打开了 verbose 级别的日志, 发现连接建立过程中 indigo 在收取 OF_HELLO 这个消息失败了, 结合代码发现是在读 socket 时发生了 EAGAIN (Resource temporary unvailable) 错误.</description></item><item><title>二叉树递归遍历的本质以及用迭代遍历精确模拟递归遍历</title><link>/posts/binary-tree-traversal/</link><pubDate>Sat, 22 Oct 2016 17:24:36 +0000</pubDate><guid>/posts/binary-tree-traversal/</guid><description>递归遍历是实现深度优先遍历的既直观又简单的方式, 二叉树递归遍历的本质其实就是在不断的压栈与出栈, 明白了这一道理之后就很容易借助栈结构来将递归遍历转换成迭代遍历.
void traverse(struct TreeNode *root) { if (root) { /* 1 */ traverse(root-&amp;gt;left); /* 2 */ traverse(root-&amp;gt;right); /* 3 */ } } 上面的 1, 2, 3 三个位置是节点访问可能发生的位置, 分别为先序, 中序, 后序遍历. 下面我们不考虑对节点数据的访问, 单独分析一下递归遍历过程的细节.
函数调用与递归的本质 要分析递归遍历的细节, 我们就要知道递归的过程中发生了什么, 而递归实际上也只是函数调用的一种, 所以我们需要先来看一下函数调用的过程中发生了什么.
当函数调用发生时, 传给该函数的参数, 以及被调函数内部的局部变量都会被压入栈中 (现在的处理器和编译器基本都支持通过寄存器传参, 这里我们没必要考虑这些个情况), 而在被调函数返回时, 这些信息又都会从栈中弹出. 递归只是一种自己调用自己的函数调用, 也是符合这个规则的.
我们以上面的 traverse(struct TreeNode *root) 方法和下面的一颗二叉树为例, 观察一下递归遍历过程中的栈变化.
A / \ B C / \ / \ D E F G 其中 A 是根节点, 遍历过程由 traverse(root) 被调用开始, 那么按照上述的规则, 会发生如下几步, 为方便起见, 栈的增长为从左往右:</description></item><item><title>关于 OpenFlow 协议中 Instruction, Action 概念的解读</title><link>/posts/openflow-instruction-action/</link><pubDate>Mon, 03 Oct 2016 21:45:30 +0000</pubDate><guid>/posts/openflow-instruction-action/</guid><description>(首发于 sdnlab: http://www.sdnlab.com/17952.html)
阅读任何一个协议都要注意的一点是这个协议中所定义的专有术语, 对这些术语的理解不到位的话也会造成对协议的理解偏差. 本文想和大家分享几个可能容易混淆的术语.
在 OpenFlow 协议文档中经常会看到这么几个词语: Instruction, Action, Apply-actions, Action Set, Action List, Clear-actions, &amp;hellip; 有点迷惑人, 实际上这里面只有两个实体的概念: Instruction 和 Action. 为了保持后文的易读性, 这两个概念分别用中文 &amp;ldquo;指令&amp;rdquo; 和 &amp;ldquo;动作&amp;rdquo; 来描述. 下文中的 &amp;ldquo;指令&amp;rdquo; 和 &amp;ldquo;动作&amp;rdquo; 都特指在 OpenFlow 协议中的含义.
指令这个词, 特指流表表项中的指令, 当某个报文匹配了这个表项之后, 表项中的指令就会被应用于这个报文; 而动作是比指令更细粒度的概念, 但它并不是局限于流表表项的概念, 动作可以独立于指令而存在, 也可以被包含在指令中, 具体说来, 我们在下流表的时候, 可以为某个表项的某种指令指定一些列的动作, 但是动作并不是只有下流表的时候才会被用到.
本文以目前较新的 Openflow 1.4 版本为准, 来分别看一下指令和动作的含义.
指令 每一个流表的表项都包含一系列的指令, 当报文匹配上了这个表项后, 这些指令就会被执行, 这些指令的执行结果有几种: 改变报文, 改变 action set, 改变 pipeline. 这些指令可以按照其执行结果的不同而分类, 不同的流表的表项包含的指令种类也不同, 前面说了指令可以包含动作, 但也并非所有种类的指令都包含动作, 下面我们一起来看一下指令的分类.
指令的分类 OpenFlow 1.</description></item><item><title>(译) 如何使用 C 语言中的 volatile 关键字</title><link>/posts/c-volatile/</link><pubDate>Sat, 07 Nov 2015 21:01:45 +0000</pubDate><guid>/posts/c-volatile/</guid><description>(原文: http://www.barrgroup.com/Embedded-Systems/How-To/C-Volatile-Keyword, 已取得翻译许可)
很多 C 程序员都不真正懂得 volatile 关键字的用法. 这无需奇怪, 因为大多数的 C 教程对 volatile 的介绍都比较简单. 这篇文章的目的就是告诉你 volatile 的正确使用方式
你有碰到过下面的几个情形吗?
代码编译运行没问题 &amp;mdash; 直到你打开了编译器优化 代码运行的很好 &amp;mdash; 直到一个中断发生 古怪的硬件驱动程序 RTOS task 各自单独运行时很好 &amp;mdash; 直到有其它 task 被 spawned 如果你碰到过上述任何一个问题, 那么就可能是你没有使用 volatile 关键字的原因. 你并不孤单, volatile 关键字为很多程序员所不熟悉. 不幸的是, 很多 C 相关的书籍都没有好好的介绍 volatile 关键字.
volatile 关键字和 const 一样, 是一个限定符, 用于一个变量被声明时. 它告诉编译器, 被声明的变量的值可能随时都会被改变 &amp;ndash; 就算使用这个变量的代码的附近 (附近有多近, 要看编译器了, 可能是同一个源文件) 没有任何修改这个变量值的语句也是如此. 给编译器的这个暗示是很严肃的, 在我们继续讲解之前, 我们先来看一下 volatile 的语法.
volatile 关键字的语法 要将一个变量声明为 volatile 的, 需要在声明时将 volatile 关键字写到数据类型关键字的前面或后面.</description></item><item><title>PAM, su 以及 wheel 用户组</title><link>/posts/pam-su-wheel-group/</link><pubDate>Sat, 02 May 2015 11:27:31 +0000</pubDate><guid>/posts/pam-su-wheel-group/</guid><description>在我的 gentoo 系统下, su 使用 pam 模组, 要求只有处于 wheel 用户组的普通用户才能够使用 su 切换到 root 用户的权限. 如果你查看 /etc/pam.d/su, 可以看到下面这一行:
auth required pam_wheel.so use_uid 但是 GNU su 不支持 wheel 用户组, 也就是说, 如果你使用的是 GNU su, 那么当你 (普通用户) 执行 GNU su 来切换到 root 权限时, GNU su 不会检查你是否是 wheel 用户组的一员, 只要你给出了 root 用户的密码就能够切换成功.
这是怎么实现的呢, 可以看出, 上面那一行 pam 规则中, flag 值用的是 required, required 这个值在这里表示不管 pam_wheel.so 模块的检查结果是成功还是失败, 后续的检查都会继续运行下去, 那么 GNU su 只要忽略 pam_wheel.so 的检查结果就可以了. 而其他的 su 实现, 则不会忽略 pam_wheel.</description></item><item><title>openocd 基础与百问网的 openjtag 介绍</title><link>/posts/openocd-and-openjtag/</link><pubDate>Sun, 12 Apr 2015 15:24:03 +0000</pubDate><guid>/posts/openocd-and-openjtag/</guid><description>使用 openocd 的话, 最好是先看看 openocd 的官方手册, 100 多页, 不需要全看, 但是基本, 核心的概念要了解, 比如说 debug adapter/adapter, interface, target, board 等.
Adapter 与 Interface 配置文件 debug adapter 或者直接叫做 adapter 呢, 就是指的你所使用的调试适配器, 一般来说就是 jtag 适配器了, 它会通过 JTAG 口与连接到开发板上, 比如 segger 家的 jlink, 开源的硬件项目 openjtag 等等.
使用不同的 adapter, 就需要在启动 openocd 时指定不同的 interface 文件, 这基本上是告诉 openocd 我的 adapter 用到了什么样的硬件, 参数是怎样的, 这样 openocd 才知道怎么和 adapter 通信, 因为 openocd 支持的 adapter 类型, 也就是支持的硬件设备类型太多了. 如果你写一个程序, 只支持 pl2303 类型的 usb-serial 设备, 那自然不需要像 openocd 那么复杂的配置了.</description></item><item><title>PEAR 与 PECL 介绍</title><link>/posts/php-pear-pecl/</link><pubDate>Mon, 09 Feb 2015 09:39:30 +0000</pubDate><guid>/posts/php-pear-pecl/</guid><description>PEAR PEAR 全称是 PHP Extension and Application Repository, 和水果 &amp;ldquo;梨&amp;rdquo; 的英文发音是相同的. PEAR 存在的目的是:
提供一个有组织结构的开源代码仓库给 PHP 用户们 提供一个代码发布以及包维护的系统 制定一份 PHP 代码风格规范 (在这里: http://pear.php.net/manual/en/standards.php) 运作 PHP Extension Community Lbrary (PECL) 姐妹组织 维护相关的网站, 邮件列表, 源镜像, PEAR/PECL 社区 PEAR 是一个社区驱动的组织, 由开发者管理. PEAR 的使命 PEAR 的使命就是为 PHP 用户提供良好可重用的组件 (避免让用户自造轮子), 以及领导 PHP 革新, 努力为 PHP 开发者提供最佳的开发体验.
由 PHP 书写的结构良好的代码库以及应用 PEAR 中的代码以 &amp;ldquo;包&amp;rdquo; 为单元. 每一个包都是一个独立维护的项目, 有专门的开发团队, 有自己的版本号, 发布周期, 项目文档, 以及与其他包的依赖关系信息.
PEAR 中的包都是以 gzip tar 档案格式发布的. 在你的系统上, 你可以使用 &amp;ldquo;PEAR installer&amp;rdquo; (http://pear.</description></item><item><title>Pathinfo 和 Nginx</title><link>/posts/pathinfo-and-nginx/</link><pubDate>Fri, 30 Jan 2015 16:27:31 +0000</pubDate><guid>/posts/pathinfo-and-nginx/</guid><description>不知为何, Nginx 中配置 PATH_INFO 似乎一直以来是一件不那么明朗的事情, 在网上搜索的话, 会搜到各种各样的配置方式. 很多都是网友们自己 &amp;ldquo;发明&amp;rdquo; 的. 各大发行版安装好了 Nginx 之后, 默认也是没有配置对 PATH_INFO 的支持的, 怎么会这样呢? 难道 Nginx 就没有一个官方的解决方案吗?
自然是有的.
PATH_INFO 是 CGI 1.1 标准中规定的一个变量, 在 www 服务器委托 CGI 脚本执行任务时, 需要传递给 CGI 脚本的信息. 这么重要的一个变量, Nginx 当然是会支持的. 参考一中就是官方的方案. 我们在这里重复一下.
首先我们知道, 在 nginx 中, 是可以使用 nginx 自带的一些命令, 给 CGI 1.1 中规定的那些变量赋值的, 而这些命令默认都位于 /etc/nginx/fastcgi.conf 或者 /etc/nginx/fastcgi_params 文件里, 在配置 fastcgi 程序处理我们的请求时, 只要在 nginx 中包含这个两个文件之一, fastcgi 程序就能够取得所需要的变量. 在我的系统上, /etc/nginx/fastcgi.conf 文件是这样的:
fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SCRIPT_NAME $fastcgi_script_name; fastcgi_param REQUEST_URI $request_uri; fastcgi_param DOCUMENT_URI $document_uri; fastcgi_param DOCUMENT_ROOT $document_root; fastcgi_param SERVER_PROTOCOL $server_protocol; fastcgi_param HTTPS $https if_not_empty; fastcgi_param GATEWAY_INTERFACE CGI/1.</description></item><item><title>Linux C 中如何正确的判断一个文件/目录文件是否存在</title><link>/posts/howto-detect-file-exists/</link><pubDate>Sun, 06 Apr 2014 14:59:00 +0000</pubDate><guid>/posts/howto-detect-file-exists/</guid><description>source: http://stackoverflow.com/questions/230062/whats-the-best-way-to-check-if-a-file-exists-in-c-cross-platform
其实不光是在 Linux 下编程, 在其他平台下我们都会有这样的需求: 我们要为应用程序创建自己的数据或者日志目录, 应用程序在每次启动时会检查文件系统中是否已经有了自己的目录, 没有的话就创建它, 有了的话就跳过这一步. 那么如何去判断文件系统中是否已经存在了要创建的目录呢?
Linux 或者 GNU C 都没有提供一个像 file_exists() 这样直观的系统调用给我们, 所以我们得通过其它的调用来达成这个目标.
实际上当我第一次要解决这个问题时, 我先 google 了一下, 这个问题在 stackoverflow 上有人问过而且非常受欢迎, 很多人对这个问题又点赞又收藏的, 自然, 这个问题也收到了不少好的答案, 这篇文章算是对这些好答案的总结和延伸.
我们先来看一个大家都应该知道的方式, 第一种方式:
fopen() fopen() 方法是流阶级的方法, 这个方法接收用户提供的文件名, 以及访问方式, 然后尝试着打开文件, 打开成功则返回 handle, 失败则返回 NULL. 因此有人提出了使用这个方法来判断指定的文件是否存在的方案:
#include &amp;lt;stdio.h&amp;gt; ... FILE *fp = NULL; fp = fopen(&amp;quot;/tmp/test/somefile&amp;quot;, &amp;quot;r&amp;quot;); if(fp) { // exists } else { // not exists } fclose(fp); 这也是 stackoverflow 上唯一一个得负分的答案, 这个方案的问题在于它没有考虑到文件权限的问题, 而 fopen() 这个函数又是如此的简单 &amp;mdash; 不管因为什么原因打开文件失败了, 它只是返回 NULL 给你, 不会提供更多的错误信息.</description></item><item><title>搞定 HP MicroServer 的 Smart Array Controller B120i 磁盘阵列, 在安装 RHEL 时</title><link>/posts/work-through-raid-in-rhel/</link><pubDate>Wed, 22 Jan 2014 09:48:00 +0000</pubDate><guid>/posts/work-through-raid-in-rhel/</guid><description>MicroServer Gen8 是 HP 服务器里较新的一个系列, 其所配备的磁盘阵列卡 &amp;mdash; Smart Array Controller B120i, 也是比较新的一种阵列卡, 目前 HP 仅提供了 RHEL, OpenSUSE, Microsoft 的驱动程序.
我们就是要在 MicroServer Gen8 上安装 RHEL6.
MicroServer Gen8 的主板上的 ROM 上搭在了一个小型的配置系统, 叫做 Intelligence Provisioning, 在这里你可以对磁盘阵列进行分区(正如 hardware raid 都会带有一个控制系统来管理自己的说法一样), 还可以配置你要安装的操作系统(不过在通过这个配置你可以安装的系统有限, 仅限于 HP 提供了阵列卡驱动的那些系统), 还带了一些系统健康状态监控的功能.
对于上述的几个可以在 Intelligence Provisoning 中配置的操作系统, MicroServer Gen8 似乎都提供了他们的安装程序, 这点比较方便, 因为在 Intelligence Provisioning 中配置好我们想安装的操作系统之后, 重启机器就回进入这个操作系统的安装界面, 然后你只需要提供操作系统的镜像就可以继续你的安装. 但是 Gen8 预置的 RHEL 操作系统安装程序却是有一个严重的不如人意的地方, 稍候我会说明这一点.
众所周知, 安装软件时, 一般来说这个软件会提供一个安装程序, 我记得 windows 下以前最火的制作安装程序的软件叫 InstallShield 不知现在还是不是最火的, 使用这个软件就可以制作出那种傻瓜化的一路下一步的软件安装程序, 而 linux 下的安装程序, 应该就得算各种包管理系统或者是.</description></item></channel></rss>